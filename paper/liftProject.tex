\documentclass[man, floatsintext]{apa7}

\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}
%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\usepackage[style=authoryear-ibid,sortcites=true,sorting=nyt,backend=biber]{biblatex}

\DeclareLanguageMapping{american}{american-apa}

\addbibresource{references.bib}

\usepackage{amsmath,amsfonts,amssymb, bm}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}


\usepackage[nameinlink,capitalize]{cleveref}

\usepackage{pgfplotstable}
\pgfplotsset{compat=1.16}
\pgfplotstableset{
	fixed zerofill,
	precision=3,
	col sep = comma,
	search path={../tables/}
}
\pgfkeys{/pgf/number format/precision={2}}%

% read all csv files

%\pgfplotstableread{mcmcSettings.csv}\tbMCMCsettings
%\pgfplotstableread{postSummaryBaseline.csv}\tbPostSummaryBaseline
%\pgfplotstableread{postSummaryExperimental.csv}\tbPostSummaryProduct
%\pgfplotstableread{postMeansCorrectedExperimental.csv}\tbCRIimprovement
%\pgfplotstableread{credibleIntervalsImprovementAllYears.csv}\tbCRIimprovement

% List all csv files here since the online LateX doesn't like them.

%mcmcSettings.csv
\pgfplotstableread{
	iter,warmup,chains,total
	60000,10000,6,300000
}\tbMCMCsettings

%postSummaryBaseline.csv
\pgfplotstableread{
	Parameter,Mean,SD,Lower,Upper
	Intercept,84.4006781257994,1.15467620910649,82.1299106371451,86.6721121643765
	Grade 11,7.67179683241211,1.1042308035749,5.50152199985356,9.82257973644633
	Grade 12,13.4561187526147,1.69206845704738,10.1530369050964,16.7906320447634
	$\sigma^2_w \, \mathrm{(school)}$,13.8144207716212,6.13710406326938,3.42643716650466,26.2693438755073
	$\sigma^2_u \, \mathrm{(student)}$,97.1350675289699,9.16849381663208,79.4750000001441,115.302127388722
	$\sigma^2_v \, \mathrm{(task)}$,10.8863691634554,3.9570990636498,4.37131576035635,18.8435503012452
	$\sigma^2_\epsilon$,198.918210347344,6.90126072443831,185.575907859527,212.598750382385
}\tbPostSummaryBaseline

%postSummaryExperimental.csv
\pgfplotstableread{
	Parameter,Mean,SD,Lower,Upper
	Intercept,76.5198727104035,5.2219862460117,64.6087711301332,88.6333473860048
	Measurement 2,9.3658352141152,1.92140066743488,5.58963143385505,13.1138395713566
	Measurement 3,9.94264986992927,1.95689846292427,6.02873389865298,13.7246696674458
	$\sigma^2_w \, \mathrm{(school)}$,86.8568892444172,213.12392067752,1.77284881700638e-10,414.621110883696
	$\sigma^2_u \, \mathrm{(student)}$,101.102847398094,23.6764647369564,57.2409705609779,148.632579566195
	$\sigma^2_\epsilon$,144.501837436296,15.9105549133032,115.001643141453,176.397842036508
}\tbPostSummaryProduct

%postMeansCorrectedExperimental.csv
\pgfplotstableread{
	Mean,what
	76.5198727104035,Exp 1 uncorrected
	85.8857079245187,Exp 2 uncorrected
	86.4625225803328,Exp 3 uncorrected
	79.5300636430171,Experiment 1
	83.5368424187699,Experiment 2
	85.4461437575531,Experiment 3
}\tbPostMeansProdCC

%credibleIntervalsImprovementAllYears.csv
\pgfplotstableread{
	Lower,Upper,mean
	0.308330070884778,1.57928803044563,0.935094630498027
	0.542479553565621,1.89013892154186,1.18928084199965
	-0.326848611252267,0.841099762073886,0.25418621150162
}\tbCRIimprovement


\hypersetup{
	colorlinks = true,
	allcolors=blue
}


\newenvironment{nscenter}
{\parskip=0pt\par\nopagebreak\centering}
{\par\noindent\ignorespacesafterend}

\newcommand\PlaceInsert[1]{%
	\smallskip%
	\begin{nscenter}
		\framebox{\Cref{#1} about here.}
	\end{nscenter}
	\smallskip%
}

\newcommand{\TbNote}[1]{%
	\begin{tablenotes}[para,flushleft]%
		{\small%
		\emph{Note. }{#1}%
		}%
	\end{tablenotes}
}

\newcommand{\TbFigNote}[1]{\figurenote{#1}}

\newcommand{\getVal}[3]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumber{\pgfplotsretval}%
}
\newcommand{\getValInt}[3]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumber[fixed, fixed zerofill=false]{\pgfplotsretval}%
}
\newcommand{\setVal}[4]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumberto{\pgfplotsretval}{#4}%
}
\newcommand{\getCI}[2]{95\% HPD [\getVal{#1}{Lower}{#2}, \getVal{#1}{Upper}{#2}]}

\newcommand{\argument}[1]{\noindent\textbf{PG:} \textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}


\newcommand{\prob}[1]{p\left(#1\right)}
\newcommand{\lik}[1]{p\left(#1\right)}
\newcommand{\data}{\mathcal{D}}
\newcommand{\model}{\mathcal{M}}
\newcommand{\prior}[1]{\pi\left(#1\right)}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\Reals}{\mathcal{R}}
\newcommand{\dnorm}[2]{\text{Normal}\left(#1,\,#2\right)}

%\newcommand{\githubLink}{\url{https://github.com/vandenman/Priors-Education}}
\newcommand{\githubLink}{\url{https://github.com/anonymousjournalsubmission2020/Prior-Information-in-Educational-Analyses}}

\title{Prior Information for Multilevel models in Educational Analyses}
\shorttitle{Prior Information in Educational Analyses}

%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\threeauthors{Don van den Bergh}{Nina Vandermeulen, Marije Lesterhuis, Sven de Maeyer,\\Elke van Steendam, Gert Rijlaarsdam}{Huub van den Bergh}
%\threeaffiliations{University of Amsterdam}{University of Antwerp}{University of Utrecht}

\authorsnames[1,2,2,2,2,2,3]{Don van den Bergh,Nina Vandermeulen, Marije Lesterhuis, Sven de Maeyer,\\Elke van Steendam, Gert Rijlaarsdam,Huub van den Bergh}
\authorsaffiliations{University of Amsterdam,University of Antwerp,University of Utrecht}
%\author[1]{Don van den Bergh\thanks{Correspondence concerning this article should be addressed to
%Don van den Bergh, University of Amsterdam, Department of Psychological Methods, Postbus 15906, 1001 NK Amsterdam, The Netherlands. E-Mail should be sent to donvdbergh@hotmail.com.}}
%\author[2]{Nina Vandermeulen}
%\author[2]{Marije Lesterhuis}
%\author[2]{Sven de Maeyer}
%\author[2]{Elke van Steendam}
%\author[2]{Gert Rijlaarsdam}
%\author[3]{Huub van den Bergh}
%\affil[1]{University of Amsterdam}
%\affil[2]{University of Antwerp}
%\affil[3]{University of Utrecht}

\newlength{\mywidth}\setlength{\mywidth}{1cm}
%\def{\mywidth}{1cm}

\abstract{%
%National assessments yield a description of the proficiency level in a domain while accounting for differences between tasks.
%For instance, in writing assessments the level of proficiency is typically evaluated with a variety topics each measured with multiple tasks.
%This enables generalizations from specific tasks to a domain.
%	
%\-\hspace{\mywidth} This contrasts with (quasi-)experimental research.
%In these studies, writing skills are often evaluated with a single task.
%Yet, conclusions about the effectivity of the treatment are formulated on the level of the domain, which is, euphemistically put, quite a stretch.
%Although conclusions drawn about the effect of the treatment are specific to the task administered, they are often generalized to the domain without any form of reservation.
%	
%\-\hspace{\mywidth} This prompts the question of whether we can use the results of national assessments about differences between tasks in the analyses of experimental studies.
%In this paper, we demonstrate how the information of a baseline data set can be used as a kind of control condition in the analysis of an experimental study.
%We adopt a Bayesian paradigm as this makes it straightforward to propagate uncertainty in the estimates of a national assessment into the analysis of the experimental study.

%National assessments yield a description of the proficiency level in a domain while accounting for differences between tasks.
%For instance, in writing assessments the level of proficiency is typically evaluated with a variety topics each measured with multiple tasks.
%This enables generalizations from specific tasks to a domain. This contrasts with (quasi-)experimental research. 
%In these studies, writing skills are often evaluated with a single task.
%Yet, conclusions about the effectivity of the treatment are formulated on the level of the domain, which is, euphemistically put, quite a stretch.
%Although conclusions drawn about the effect of the treatment are specific to the task administered, they are often generalized to the domain without any form of reservation. This prompts the question of whether we can use the results of national assessments about differences between tasks in the analyses of experimental studies.
%In this paper, we demonstrate how the information of a baseline data set can be used as a kind of control condition in the analysis of an experimental study.
%We adopt a Bayesian paradigm as this makes it straightforward to propagate uncertainty in the estimates of a national assessment into the analysis of the experimental study.

National assessments yield a description of the proficiency level in a domain while accounting for differences between tasks.
For instance, in writing assessments the level of proficiency is typically evaluated with a variety of topics each measured with multiple tasks.
This enables generalizations from specific tasks to a domain. This contrasts with (quasi-)experimental research.
In these studies, writing skills are often evaluated with a single task.
Yet, conclusions about the effectiveness of the treatment are formulated on the level of the domain, which is, euphemistically put, quite a stretch.
Although conclusions drawn about the effect of the treatment are specific to the task administered, they are often generalized to the domain without any form of reservation. This prompts the question of whether we can use the results of national assessments about differences between tasks in the analyses of experimental studies.
In this paper, we demonstrate how the information of a baseline data set can be used as a kind of control condition in the analysis of an experimental study.

}

\keywords{Prior information, Baseline comparison, Bayesian inference}

\date{}

\graphicspath{{../figuresFlattened/}}

\begin{document}

\maketitle
%\renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}

In many countries, the achievements of students are monitored in so-called national assessments.
For instance, NAEP in the US, PEIL in the Netherlands (or international assessment programs like IEA or PIRLS) measure students' achievements at regular intervals to gain information on changes in achievement over time (or changes in differences between countries).
Although the results of these assessments often inform policymaking, the data are seldom used in educational research even though there are ample opportunities.

A common denominator in national assessments is for that all subject domains measurements are based on a domain definition, based on an analysis of that domain.
Therefore, all students write multiple texts in the case of a writing assessment.
This is a necessity if one wants to describe the level of achievements covering a whole domain while generalizing over specific assignments at the same time.
For writing in the Netherlands, for instance, students took a sample of 3 out of 21 assignments \parencite{zwarts1990balans} via an overlapping design to optimize the relation between testing time per individual while allowing for conclusions at the population level at the same time.

If we contrast experimental studies with national assessments it is apparent that in many experimental studies the measurements are not as varied as in national assessments.
In the vast majority of experimental studies on writing, students write one text as a pretest and one text as a posttest \parencite[e.g.,][]{graham2014conducting}.
Based on these texts we are proned to draw conclusions on changes in the writing skills of students.
However, it is well documented that differences between different types of writing assignments can be large \parencite[e.g.,][]{bouwer2015effect}.
We thus can hardly make inferences based on only one writing assignment.
Although many researchers are aware of the limited generalizability of single-task experiments, in most studies it is often infeasible that students write more tasks.

So, on the one hand, there is much information on levels of achievement (at certain levels of education) from assessments, and on the other hand in most experimental studies, we must rely on relatively small samples of participants and relatively narrow samples of domain specific tasks. Therefore, one could wonder why do we not use the information from large scale assessments? Can this information from assessments be put to use in our experimental studies?

In experimental studies, quite often the goal is to show that the increase in achievements due to an experimental manipulation exceeds `natural' growth.
Usually, this is done by comparing an experimental condition to a control condition, where the control condition represents the expectation given the `natural' growth of achievement.
This is possibly inefficient, as students are measured repeatedly to obtain studies that have sufficient power to draw conclusions.
One alternative to increase the power of studies is by enriching the statistical analyses with prior results \parencite{rijlaarsdam2011application}.
In contrast to control conditions, national assessments consist of more observations on a larger variety of tasks and therefore provide a much richer account of the general level of achievement.
Thus, the idea is to use the data from a national assessment as prior knowledge on the general level of achievement instead of a control group.

Unfortunately, no straightforward method exists to incorporate prior information into (frequentist) analyses. 
Ideally, the raw data from prior studies are included in the analyses as a benchmark comparison, but this is often impossible for practical (and privacy) reasons.
Alternatively, prior knowledge can be represented by treating the prior results as population values and experimental results can be tested against these values.
However, this approach seems far from ideal, as measurement error and uncertainty in the prior results are completely ignored.
Although such uncertainties could be introduced using standard errors, many types of frequentist analyses are not equipped for such procedures.

In this respect, Bayesian analyses of prior results from the data might be preferable. Bayesian statistics offer a rigorous and consistent approach to quantify uncertainty in statistical analyses. In Bayesian inference, prior knowledge (or a lack thereof) is represented by probability distributions, which describe all uncertainty about the quantities of interest. Upon observing the data, prior knowledge is updated to posterior knowledge, which is again represented by probability distributions. Key is that these probability distributions provide a complete account of the uncertainty. Therefore, Bayesian inference is an ideal vehicle to reuse findings from prior analyses into future studies, while accounting for the uncertainty in these prior results. In educational research, there is an abundance of data, but results from the analyses are rarely used in the analysis of new studies.

The outline of this paper is as follows. 
First, we introduce a large data set on writing instruction in high school that serves as a baseline data set. 
Using this baseline data set, we provide a brief explanation of Bayesian statistics, before analyzing the data with a multilevel model. 
Next, we analyze a follow-up data set from an experiment and relate the parameter estimates from the baseline analysis to those of the follow-up to demonstrate how findings from prior large-scale assessments can provide a baseline in experimental research. 
We conclude by discussing the widespread applicability and benefits of this approach and the limitations concerning the validity of this approach.


\section{Baseline Data Set}
\noindent The baseline data set was collected to investigate the writing quality of students in the tenth, eleventh, and twelfth grade of high school. Here we provide some information about the data collection and some descriptives of the data.

Schools were selected at random by creating three lists of schools. First, a school in the first batch was approached for participating in the study. If this school did not reply or refused, a school in the second batch was selected at random. If the second school did not participate a school from the third batch was approached.

In total, the writing quality was measured for 625 students, nested in 43 schools. To assess between-task variance, 32 different tasks were administered of which each students made four. Not all students made all tasks, 497 students made four tasks and 128 students made three or fewer tasks. The minimum amount of students per task was 62 whereas the maximum was 84. 
A benchmark rating procedure was used to assess the students' texts.
This procedure entails that texts are rated holistically by comparing them to five benchmark texts at intervals of 1 SD.
Benchmark rating proved to be a reliable rating method in several previous studies on writing \parencite{blok1986essay, de2016student, rietdijk2017improving,bouwer2017tekster}.
An overlapping rater team design with a total of 48 raters was applied \parencite{vandenbergh1989method}.
Every text was rated by a jury of three raters; average jury reliability was 0.65. The text's final score consisted of the average of the three separate scores.

The data contain an obvious nested structure: observations are nested within students and tasks, and students are nested within schools.
Students took a sample of four tasks out of 32 tasks developed for this writing assessment.
Figure~\ref{fig:baselineDescriptives} shows the observed differences between grades, tasks, and students.
\PlaceInsert{fig:baselineDescriptives}
\begin{figure}[!ht]
	\caption{Box and Whiskers Plot of Student Performance on each Task for the three Grades Measured}
	\fitfigure{descriptivesBaseline.pdf}
	\figurenote{The grade is indicated above each panel and the task code is shown on the x-axis. There is substantial variance in student performance between tasks and within tasks, and student performance appears to increase in successive grades.}
	\label{fig:baselineDescriptives}
\end{figure}
The observations of text quality cannot be considered as independent.
Scores of students in the same school might be more alike than scores of students from different schools.
Likewise, scores on the same task might be more alike than scores on different writing tasks.
Therefore, a cross-classified multilevel model is in operation.
If $y_{(ij)k}$ is the score of student $i$ ($i = 1, 2, 3, ...., I_k$) on task $j$ ($j = 1, 2, ..., J_i$) in school $k$ ($k = 1, 2, ...., K$), we can write the model that we will analyze as a function of student, task, school, and grade:
\begin{align*}
	y_{(ij)k} &= \beta_0 + \beta_1 \times [\mathrm{Grade}_{(ij)k}=11] + \beta_2 \times [\mathrm{Grade}_{(ij)k}=12]\\
			  &+ w_{00k} + u_{i0k} + v_{0j0} + \epsilon_{(ij)k}.
\end{align*}
The model consists a fixed part, the first line, and a random part, the second line. In the fixed part, $\mathrm{Grade}_{(ij)k}$ is an indicator matrix for students' grade (e.g., for students in grade 11 $[\mathrm{Grade}_{(ij)k}=11] = 1$, otherwise 0). The intercept $\beta_0$ represents the mean writing score in Grade 10. Consequently, the regression weights $\beta_1$ and $\beta_2$ represent the difference in mean writing score between grade 11 and 10, and grade 12 and 10 respectively. In the random part four residual scores are distinguished, all of which are assumed to be normally distributed with an expected value of 0. The first residual ($w_{00k}$) captures the difference between the average of a school and the intercept. The second residual ($u_{i0k}$) captures that the average score of student $i$ in school $k$ can deviate from the schools' average. The third residual ($v_{0j0}$) captures that some tasks might be more difficult than other tasks. The fourth residual ($\epsilon_{(ij)k}$) indicates the deviation of the score of task $j$ of the average of student $i$ in school $k$. Usually, the variance of this term is interpreted as random noise.


\subsection{Bayesian Inference}
\pgfmathsetmacro{\zCrit}{1.959964}% qnorm(0.975)

This section aims to give a brief introduction to Bayesian inference with an emphasis on the problem at hand. For a more elaborate introduction to Bayesian inference, see the special issue in \emph{Psychonomic Bulletin \& Review} which provides tutorials and guidance for aspiring Bayesians \parencite{VandekerckhoveEtAl2018SI}. The choice for a Bayesian analysis is motivated by the fact that Bayesian inference is naturally accompanied by uncertainty estimates, as is explained later. Thus, the estimates of a baseline study and an experimental study can be compared while accounting for the uncertainty in both sets of estimates.

Bayesian inference is centered on the updating of beliefs.
Prior beliefs express our uncertainty about parameter values before observing any data whereas posterior beliefs express our uncertainty about the parameter values after updating the prior beliefs with the observed data.
For any parameter in a given statistical model $\model$, the values this parameter can take are assigned a prior belief.
Beliefs are represented with a probability distribution, usually called the prior distribution for the prior beliefs and posterior distribution for posterior beliefs.
For example, in a multilevel model, the intercept $\beta_0$ can be assigned a normal distribution as prior distribution with mean 0 and variance 1.
Then a-priori the most likely values for the intercept are near 0 and about 95\% of the prior mass lies within \pgfmathprintnumber{-\zCrit} and \pgfmathprintnumber{\zCrit}.

The key step in Bayesian inference is to use the data $\data$ to update the prior beliefs to posterior beliefs.
The procedure for updating the prior distribution to a posterior distribution is given by Bayes' theorem:
\begin{align*}\label{eq:BayesTheorem}
\underbrace{\prob{\bm{\beta} \mid \data , \model}}_{\text{Posterior}}
&=
\overbrace{\prob{\bm{\beta}\mid \model}}^{\text{Prior}}
\,\,
\underbrace{\overbrace{
		\frac{\lik{\data \mid \bm{\beta}, \model}}{\prob{\data \mid \model}}
	}^{\text{Likelihood}}}_{\substack{\text{Marginal}\\ \text{Likelihood}}}.
\end{align*}
Here, $\bm{\beta}$ represents all parameters in the model. The prior distribution of the parameters is updated through the likelihood of the statistical model. The likelihood is divided by the marginal likelihood so that the posterior distribution is a proper probability distribution (i.e., it integrates to 1). The posterior distribution is key for parameter estimates. For instance, if a single estimate for a parameter is desired, one could use the mean of the posterior distribution. Other often-used point-estimates are the posterior mode and posterior median. Simultaneously with obtaining the posterior, a measure of uncertainty for each parameter is obtained. Since the posterior distribution is a proper probability distribution, we can use it to draw inferences about the parameters. For example, consider the posterior distribution for the intercept, $\prob{\beta_0 \mid \data , \model}$. Questions such as ``Given that we have seen the data, what is the probability that the intercept is larger than 0?'' can be answered by computing $\prob{\beta_0 > 0 \mid \data , \model}$. Likewise, if we find a lower bound $LB$ and upper bound $UB$ for the intercept $\beta_0$ such that $\prob{ LB \leq \beta_0 \leq UB \mid \data , \model} = 0.95$, we can claim: ``Given that we have seen the data, we are $95\%$ confident that the population value of the intercept lies between $LB$ and $UB$.'' This interval is known as the Bayesian $95\%$ credible interval. Another often-used Bayesian uncertainty interval is the 95\% highest posterior density interval (HPD), an interval that contains 95\% of the posterior mass and has the highest probability density.

\subsubsection{Approximations to Posterior Distributions}
Bayes' theorem may appear straightforward, however, in practice a model may contain a large number of parameters which complicates studying the posterior distribution analytically. 
This holds in particular for multi-level models, as the number of dimensions equals the sum of the number of random effects, fixed effects, and variances.
Rather than studying the mathematical form of the posterior, it is easier to simulate random values from the posterior distribution and to use these for inference.
Such simulation methods are commonly referred to as Markov chain Monte Carlo (MCMC).
The idea is that instead of computing a statistic of the posterior distribution in closed form, we draw a large number of random observations from the posterior distribution and use a sample estimator to approximate the statistic of the distribution.
For example, if we are interested in the posterior mean of the intercept, we simulate many observations from the posterior distribution and use the sample mean of these observations to approximate the posterior mean of the intercept.
Likewise, to compute the posterior probability that an intercept $\beta_0$ is positive, $\prob{\beta_0 > 0 \mid \data , \model}$, we take the proportion of MCMC samples where $\beta_0$ is positive.
This procedure is akin to how applied scientists randomly sample participants from a population and then generalize the sample statistics to the population, although with MCMC it is often easy to draw enormous samples to obtain a near-perfect approximation.

\subsection{Statistical Software}
%\pgfplotstableread{mcmcSettings.csv}\tbMCMCsettings

All analyses were done in R \parencite{R}.
The R package \code{brms} was used for Bayesian multilevel analyses \parencite{burkner2017brms}.
The R package \code{brms} is a convenient front-end for the probabilistic programming language Stan, which is software for general-purpose Bayesian inference \parencite{carpenter2017stan}.
R code for all analyses is available at \githubLink{}.
For all analyses, we used six MCMC chains to assess convergence.
Convergence was assessed using the $\widehat{R}$ statistic \parencite{vehtari2019rank}.
In line with the recommendations by \textcite{vehtari2019rank}, we tweaked the parameters of the Stan algorithm such that the $\widehat{R}$ is less than 1.01 and the rank-normalized effective sample size is larger than 400.
Per chain, we simulated \getValInt{0}{iter}{\tbMCMCsettings} samples and discarded the first \getValInt{0}{warmup}{\tbMCMCsettings} as warmup samples.
In total, results in Tables and Figures are based on \getValInt{0}{total}{\tbMCMCsettings} samples of the posterior distribution.

We used the default prior distributions of the R package \code{brms} for all parameters. That is, the standard deviations of the random effects and the residual were assigned a half t-distribution with a mean of 0, scale of 18, and 3 degrees of freedom. For the fixed effects we used a Cauchy distribution with location 0 and scale 1.

\section{Baseline Analysis}

% setup table
%\pgfplotstableread{postSummaryBaseline.csv}\tbPostSummaryBaseline

We summarized the posterior distribution in Table~\ref{tb:baselineSummary}.
This shows that the average text quality of students in grade 10 is estimated at \getVal{0}{Mean}{\tbPostSummaryBaseline}.
The 95\% highest posterior density (HPD) credible interval ranges from  \getCI{0}{\tbPostSummaryBaseline}.
Students in grade 11 performed on average about \getVal{1}{Mean}{\tbPostSummaryBaseline} points better (\getCI{1}{\tbPostSummaryBaseline}) than students in grade 10.
Likewise, students in grade 12 performed on average about \getVal{2}{Mean}{\tbPostSummaryBaseline} points better (\getCI{2}{\tbPostSummaryBaseline}) than students in grade 10.
The estimated variance between schools (\getVal{3}{Mean}{\tbPostSummaryBaseline}), students within school (\getVal{4}{Mean}{\tbPostSummaryBaseline}), and tasks (\getVal{5}{Mean}{\tbPostSummaryBaseline}) clearly deviate from 0.
Since the data set contained such a large variety of schools and tasks, these findings likely generalize over tasks.

\PlaceInsert{tb:baselineSummary}
\begin{table}[htpb]
	\caption{Summary of the Posterior Distribution for the Baseline Data Set}
	\label{tb:baselineSummary}
	\begin{nscenter}
	\pgfplotstabletypeset[
	column type=r,
	every head row/.style={
		before row={
			\toprule
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{95\% HPD} \\
			\cmidrule[0.4pt]{4-5}
		},
		after row=\midrule,
	},
	every last row/.style={
		after row=\bottomrule
	},
	columns/Parameter/.style={string type}
	]\tbPostSummaryBaseline
	\end{nscenter}
	\TbNote{The first column shows the parameter, the second the posterior mean for that parameter, the third the posterior standard deviation and the last two columns show the 95\% higher posterior density interval. Grade 11 and 12 represent the improvement relative to grade 10 (the intercept). The posterior standard deviation may be interpreted as a standard error.}
\end{table}

Figure~\ref{fig:baselinePosteriorTextQuality} visualizes the improvement in text quality across grades. To obtain the posterior distributions for grades 11 and 12, we add the posterior distribution of the intercept to that of the improvement in Grade 11 and Grade 12.

\PlaceInsert{fig:baselinePosteriorTextQuality}
\begin{figure}[!ht]
	\caption{Posterior Distribution of Text Quality in Grades 10, 11, and 12}
	\fitfigure{posteriorTextQualityBaseline}
	\figurenote{Posteriors distributions for grades 11 and 12 are obtained by adding the MCMC samples of the intercept to the MCMC samples for the improvement of the respective grade.}
	\label{fig:baselinePosteriorTextQuality}
\end{figure}

The descriptive plot in Figure~\ref{fig:baselineDescriptives} suggested that the scores of students depend on the task they made. This is further confirmed by the posterior mean of between task variance \getVal{5}{Mean}{\tbPostSummaryBaseline} (\getCI{5}{\tbPostSummaryBaseline})) which is far away from 0. This shows that differences between tasks are substantial and that some tasks are systematically more difficult than others. Consequentially, a students' domain score depends on the task they made.

\section{Application to an Experimental Analysis}

In this section, we demonstrate how the findings based in the baseline analyses can be used as prior information to assess the impact of an intervention in an experimental analysis.

\subsection{Experimental Data Set}
Data were collected from 89 students of two high-schools in the Netherlands.
Students made three writing tasks in one week; one on Monday, Wednesday, and Friday.
Prior to writing the second and third text, the participants received feedback on their previously written text.
As part of the feedback, students received annotated exemplar texts selected from the national baseline; these are texts that are representative of a specific position on the benchmark scale, accompanied with a description for several text quality aspects.
Students could compare and contrast their own text with the exemplar texts of students with the same or a better score.
Figure~\ref{fig:productDescriptives} depicts the data from the experiment per measurement occasion.

\PlaceInsert{fig:productDescriptives}
\begin{figure}[!ht]
	\caption{Box and Whiskers Plot of Student Performance on the three Measurement Occasions}
	\fitfigure{descriptivesExperimental}
	\figurenote{Grey points represent the raw scores on text quality. Quasi-random jitter was added to the x-coordinates of the points to avoid visual clutter. The average performance clearly increases from measurement one to two, but it is hard to quantify the improvement without a reference group.}
	\label{fig:productDescriptives}
\end{figure}

\subsection{Experimental Analysis}
%\pgfplotstableread{postSummaryExperimental.csv}\tbPostSummaryProduct

A typical analysis for this data set is almost identical to that of the baseline data set, except that here we estimate differences between measurements, which might be contaminated with differences due to tasks. Since each student took only one task at each measurement occasion, the between-task variance cannot be estimated. Thus $y_{hik}$ is the observation of measurement $h$ ($h=1, 2, 3$) of student $i$ ($i=1, ..., K_i$) in school $k$ ($k = 1, 2$). The multilevel model thus becomes:
\begin{align*}
%y_{(ij)k} = \beta_0 + \beta \times \mathrm{Measurement}_{ijk} +  [w_{00k} + u_{i0k} + \epsilon_{ijk}].
y_{hik} &= \beta_0 + \beta_1 \times [\mathrm{Measurement}_{hik} = 2] + \beta_2 \times [\mathrm{Measurement}_{hik} = 3]\\
        &+ w_{00k} + u_{0ik} + \epsilon_{hik}.
\end{align*}
%Here, $y_{hjk}$ is the observation of student $i$ on measurement $j$ in school $k$. 
The fixed part consists of an intercept ($\beta_0$) that represents the mean writing score on the first measurement and two fixed effects that capture the difference in mean writing score between subsequent measurements and the first measurement ($\beta_1$ and $\beta_2$).
The random part consists of a random intercept for school ($w_{00k}$), a random intercept for person within school $u_{i0k}$ and a residual $\epsilon_{hik}$.
%\begin{align*}
%y_{psm} &= \beta_0 + \beta_p + \beta_s + \beta_m  + \epsilon_{pstg}.
%\end{align*}
%Here, the observation of student $p$ in school $s$ on measurement $m$ is denoted $y_{psm}$. The score consists of an intercept $\beta_0$, a random intercept for person $\beta_p$ and school $\beta_s$, and a fixed effect of measurement $\beta_m$.
As for the baseline analysis, we summarize the posterior distribution of the multilevel model using the mean, standard deviation, and HPD in Table~\ref{tb:productPosteriorSummary}. This shows that the average text quality is estimated at \getVal{0}{Mean}{\tbPostSummaryProduct} (\getCI{0}{\tbPostSummaryProduct}). At the second measurement occasion, students performed on average about \getVal{1}{Mean}{\tbPostSummaryProduct} points better (\getCI{1}{\tbPostSummaryProduct}) than at intake. At follow up, students' improvement was estimated at \getVal{2}{Mean}{\tbPostSummaryProduct} (\getCI{2}{\tbPostSummaryProduct}). A bivariate scatterplot for the parameters in Table~\ref{tb:productPosteriorSummary} is shown in Figure~\ref{fig:productPosteriorDescriptives}.

\PlaceInsert{tb:productPosteriorSummary}
\begin{table}[htpb]
	\caption{Summary of the Posterior Distribution for the Experimental Data Set}
	\label{tb:productPosteriorSummary}
	\begin{nscenter}
		\pgfplotstabletypeset[
			column type=r,
			every head row/.style={
				before row={
					\toprule
					\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{95\% HPD} \\
					\cmidrule[0.4pt]{4-5}
				},
				after row=\midrule,
			},
			every last row/.style={
				after row=\bottomrule
			},
			columns/Parameter/.style={string type}
		]\tbPostSummaryProduct
	\end{nscenter}
	\TbNote{The first column shows the parameter, the second the posterior mean for that parameter, the third the posterior standard deviation and the last two columns show the 95\% higher posterior density interval. The improvement of measurement 2 and 3 is relative to the intercept (measurement 1). The posterior standard deviation may be interpreted as a standard error.}
\end{table}

The estimated improvement across measurement occasions is shown in Figure~\ref{fig:productPosteriorTextQual}. Apparent is that students perform better at posttest than at pretest and that the difference between follow up and posttest appears negligible.

\PlaceInsert{fig:productPosteriorTextQual}
\begin{figure}[!ht]	
	\caption{Posterior Distribution of Text Quality at each Measurement Occasion of the Product Data Set.}
	\fitfigure{posteriorTextQualityExperimental.pdf}
	\label{fig:productPosteriorTextQual}
\end{figure}
At this point in the analysis, drawing conclusions about the effect of the treatment is problematic because there is no control group. 
Thus, the improvement of the students cannot solely be attributed to just the intervention but might be caused by differences in difficulty between tasks.

\subsection{Relating Baseline Results to the Analysis of an Experimental Study}
%\pgfplotstableread{postMeansCorrectedExperimental.csv}\tbPostMeansProdCC

Ideally, we directly compare the difference in text quality between measurement occasions in the experimental study. However, interpreting these differences is not straightforward as the contamination of task effect and measurement occasion makes this impossible. To make the differences between measurement occasions interpretable we need to correct these for task difficulty. As the baseline study provides estimates of task difficulty, a correction is self-evident.  We can correct students' scores in the experimental study by subtracting the estimated task effect in the baseline study. As a consequence, the corrected posterior means for each measurement occasion changed slightly, see Figure~\ref{fig:comparePostTextQual} (from \getVal{0}{Mean}{\tbPostMeansProdCC} to \getVal{3}{Mean}{\tbPostMeansProdCC} for measurement 1, from \getVal{1}{Mean}{\tbPostMeansProdCC} to \getVal{4}{Mean}{\tbPostMeansProdCC} for measurement 2, and from \getVal{2}{Mean}{\tbPostMeansProdCC} to \getVal{5}{Mean}{\tbPostMeansProdCC} for measurement 3). Note that a direct comparison is possible because the rating procedure of the experimental study is based on the rating procedure of the baseline study.
\PlaceInsert{fig:comparePostTextQual}
\begin{figure}[!ht]
	\caption{Posterior Distributions for each Grade in the Baseline Study and each Measurement Occasion in the Experimental Study.}
	\fitfigure{comparePosteriorTextQuality.pdf}
	\figurenote{The posterior distributions of the baseline study are narrower because they are based on more observations. We subtracted the estimated average task effect of each task category in the baseline study from the posteriors distributions in the experimental study to correct these for the task effect.}
	\label{fig:comparePostTextQual}
\end{figure}

From Figure~\ref{fig:comparePostTextQual} we can infer that the corrected difference between measurement 1 and measurement 2 in the experimental study is almost as large as the difference between grade 10 and 11 in the baseline study. Hence, there is a substantial difference between both measurements and thus an experimental effect. By comparison, the difference between measurement 2 and measurement 3 is much smaller. Of course, this is not a statistical test of significance. Typically, such a test should account for between-task variance. To obtain an estimate for the magnitude of between-task effects we can use the estimates of the baseline study to simulate a distribution of task difficulty. Next, we can compute the probability that the observed difference between measurement occasions in the experimental study is due to differences between tasks.

Since multilevel models typically assume that the random effects follow a normal distribution with mean 0 we simulate a large number of task effects from a normal distribution with mean 0. As variance for this normal distribution, we use the posterior samples for the between-task variance, to propagate the uncertainty in this parameter into the distribution over task-effects.
%\footnote{Essentially, the effects of these new random tasks are drawn from the posterior predictive distribution of the baseline study.} 
In total \getValInt{0}{total}{\tbMCMCsettings} task-effects were sampled (the same amount as MCMC samples). Next, we can visualize the posterior distribution of improvement between measurement occasions and contrast this with the distribution over task-effects, as shown in Figure~\ref{fig:posteriorImprovement}.
\PlaceInsert{fig:posteriorImprovement}
\begin{figure}[!ht]
	\caption{Distribution of the Effect of a Random Task versus the Posterior Distribution of the Estimated Progress in the Experimental Data}
	\fitfigure{compareTaskEffects.pdf}
	\figurenote{The grey density represent the distribution of the effect of a random task. The blue density in the left panel is the posterior distribution of improvement between the first and second measurement; the purple density in the middle panel is the posterior distribution of improvement between the first and third measurement; the dark blue density in the right panel is the posterior distribution of improvement between the second and third measurement. The probability that a random task is larger than the improvement is shown above each panel.}
	\label{fig:posteriorImprovement}
\end{figure}

The left and middle panel in Figure~\ref{fig:posteriorImprovement} contrast measurement occasions 2 and 3 against intake.
The improvement appears to exceeds what would be expected of a random task-effect.
The right panel contrasts measurement 2 with measurement 3. Here, the improvement seems indistinguishable from a random task-effect.


The above results show that the observed effect between measurement occasions 1 and 2 is larger than can be expected from any given task.
However, this does not provide a straightforward manner to interpret the magnitude of this effect.
To obtain a measure that is easy to interpret, we again compare the results to that of the baseline.
From the baseline study, we obtained a posterior distribution that quantifies students' improvement between grade 10 and grade 11, accounting for differences between tasks (i.e., parameter Grade 11 in Table~\ref{tb:baselineSummary}).
Next, we take the posterior samples for the effect of measurement in the experimental study (Parameter Measurement 2 in Table~\ref{tb:productPosteriorSummary}) and divide these by the samples of the baseline study.
The resulting posterior distribution expresses the progress of students in the experimental study in baseline study years and provides a practically intuitive interpretation for the effect size.
The resulting posterior distributions are shown in Figure~\ref{fig:improvementInYears}.

\PlaceInsert{fig:improvementInYears}
% these cannot be generated inside the caption
%\pgfplotstableread{credibleIntervalsImprovementAllYears.csv}\tbCRIimprovement
\setVal{0}{Lower}{\tbCRIimprovement}{\criLowerA}
\setVal{0}{Upper}{\tbCRIimprovement}{\criUpperA}
\setVal{1}{Lower}{\tbCRIimprovement}{\criLowerB}
\setVal{1}{Upper}{\tbCRIimprovement}{\criUpperB}
\setVal{2}{Lower}{\tbCRIimprovement}{\criLowerC}
\setVal{2}{Upper}{\tbCRIimprovement}{\criUpperC}
\begin{figure}[!ht]
	\caption{Improvement in the Experimental Study relative to the Improvement between Grades 11 and 10}
	\fitfigure{improvementInAllYears.pdf}
	\figurenote{The left panel shows the posterior distribution for the improvement between measurement 1 and measurement 2 divided by the improvement from grade 10 to 11 (95\% HPD [$\criLowerA$, $\criUpperA$]). The middle panel shows the improvement from measurement 1 to measurement 3, standardized in the same manner (95\% HPD [$\criLowerB$, $\criUpperB$]). The right panel shows the improvement between measurement 2 and 3  (95\% HPD [$\criLowerC$, $\criUpperC$]). The shaded areas under the curve represent 95\% HPD intervals.}
	\label{fig:improvementInYears}
\end{figure}

In the left panel of Figure~\ref{fig:improvementInYears}, the posterior has a mean \getVal{0}{mean}{\tbCRIimprovement} (\getCI{0}{\tbCRIimprovement}), which indicates that the students appear to have gained almost a year in ability between the two measurements. In the middle panel, the posterior mean is \getVal{1}{mean}{\tbCRIimprovement} (\getCI{1}{\tbCRIimprovement}) which indicates that this improvement is still present at the third measurement. In the rightmost panel, the mean improvement is \getVal{2}{mean}{\tbCRIimprovement} (\getCI{2}{\tbCRIimprovement}) which is much smaller and indicates less improvement between measurement occasions two and three.

\section{Discussion}
In this paper, we related the results of an experimental study to those of a baseline study.
The post-intervention improvement in the experimental study exceeded the differences between random tasks in the baseline study.
Therefore, we interpret the effect of the intervention as significant.
We further quantified this effect by expressing the improvement in the number of school years.

If a control group is missing, the task-effect cannot be disentangled from the effect of an intervention.
However, by relating the increase in performance to estimates of the between-task variance in a baseline study, we can compute how probable it is that improvement across measurements is a task-effect.
This method could provide a point of reference for studies without a control group and may help discern between statistically significant effects and practically relevant effects and relate the effects different studies to each other \parencite{fan2001statistical, hojat2004visitor}.


A comparison with a baseline study can also enrich the results of studies with a control group.
For example, if a study administers a smaller variety of tasks than a baseline study, a comparison can provide a clearer assessment of the generalization of an effect over tasks.

Comparing the distribution of task-effects in an experimental study to that of a baseline study relates to approaches of statistical tests for equivalence, such as TOST \parencite{lakens2017equivalence}, ROPE \parencite{kruschke2011bayesian}, and interval Bayes factors \parencite{MoreyRouder2011}.
These three approaches have in common that a researcher specifies some minimal effect size below which an effect is practically equivalent to zero.
These tests have also in common is that they provide little guidance on how to determine such a minimal effect size.
In contrast, our approach can be seen as deriving this minimal effect size from a baseline study (e.g., the effect size that is sufficiently implausible to be caused by between task effects.)

Here, we opted to speak of significance when the posterior probability that the observed task-effect is larger than that of a random task is less than $0.05$. This choice is arbitrary and other motivations have been suggested \parencite{McShane2017abandon, BenjaminEtAl2018}.

We chose for a Bayesian analysis because it allows us to account for the uncertainty in the estimates of the baseline study when comparing these to the results in the experimental study.
In a frequentist analysis, it is also possible to do this to some extent.
For example, a distribution of effect sizes can be approximated using a normal distribution with as mean the point estimate and as standard deviation the corresponding standard error.
However, such an approach may introduce difficulties for variance parameters as this puts nonzero mass on negative values.
The Bayesian paradigm avoids this scenario automatically through the prior.



\subsection{Limitations}
As is typical for quasi-experimental research in educational settings, there was no random assignment in the experimental study.
Therefore, the usual limitations of quasi-experimental research apply; it is possible that the observed differences between measurement occasions are caused by a confounding variable rather than the intervention because students are not randomly assigned to either the experimental or baseline condition/ study.
Caution should be exercised in interpreting the conclusions based on a comparison with a baseline study as causal.

A key assumption of multi-level models is that task-effects are, at least asymptotically, normally distributed.
If normality is violated then the probabilities shown in Figure~\ref{fig:posteriorImprovement} could be biased.
Here, we briefly outline an argument on why the task effects are likely approximately normally distributed.
Note that a naive estimator for the effect of a task is simply the mean of the students' scores on that task.
Although this estimator is unbiased, much better estimates can be obtained by accounting for the hierarchical structure \parencite[e.g., see][]{EfronMorris1977}.
Since the naive estimator is an average the central limit theorem applies and thus the distribution of task-effects converges asymptotically to a normal distribution (under mild regularity conditions).

Another avenue for incorporating the results of a baseline study into the analysis of an experimental study is through the prior distribution.
The posterior distribution of the baseline study could serve as the prior distribution for the experimental study.
Although this is conceptually straightforward, we did not do so for two reasons.
First, to obtain exact approximations to the posterior, the analyst of the experimental study must have the original data to obtain posterior distributions for the baseline data set.
In practice, it is unlikely that an analyst has access to a baseline data set which limits the applicability of the method.
It is possible to approximate the marginal posterior distributions using some parametric family of distributions, which can then be published and used in experimental studies.
However, these approximations will likely ignore the correlations and other higher-order moments in the posterior distribution.
The consequences of ignoring the higher-order moments in the posterior distribution are simply unknown.
Second, the benefit of informed priors is unclear, as the data typically overwhelm the influence of the prior distribution, barring extreme cases \parencite{Lynch2007}.
Thus, since the inferences done in the paper are based solely on the posterior distribution, the influence of the prior distribution is likely negligible.

\subsection{Recommendations}
Prior information can enrich the statistical analyses and provide more insight into the data.
Here, we outline three recommendations for those who wish to apply our method for incorporating prior information in practice.

A key requirement for comparing results from a large scale assessment with those from an experimental study is that the data are comparable.
Whether the data are comparable hinges on the validity of the measurement instruments.
That the instruments measure what they are supposed to measure (the traditional definition of validity) is not as important as that they measure the same construct.
If the baseline study measured different constructs than the experimental study, for instance, because they used different measurement instruments, then a comparison is unintelligible and thus meaningless.
Thus, we recommend using the same measurement instruments as those used in a baseline assessment if we want to make a meaningful comparison with baseline results.

The use of a baseline study instead of a control group opens up new avenues for designing experimental studies.
Currently, researchers tend to allocate about half of the available resources to a control group and the other half to an experimental group.
However, since the experimental group can now be related to a baseline study, it becomes possible to allocate funds to a theoretically competing theory, rather than to a control group.

The use of our method is subject to large scale assessments publishing their results.
It is key that those studies either disclose the raw data or that they publish the marginal posterior distributions of the parameters.
If the results of large scale assessments are not available as a benchmark, then it is inoperable to use them to inform the analysis of experimental studies.

In sum, we related the results from a baseline study to the analysis of an experimental study that lacked a control group.
This allowed us to determine whether the differences between measurements in the experimental group exceeded what would be expected from between task variance.
Altogether, this may help to place effect sizes of experimental studies in a broader context.

\printbibliography

\appendix\section{Convergence Diagnostics and Visual Summaries}
%\newpage

\begin{figure}
	\caption{Visual Summary of the Posterior Distributions for the Group Level Effects of the Baseline Data Set}
	\fitfigure{posteriorDescriptivesBaseline}
	\figurenote{The strips above and right of the figures indicate the parameters compared. Figures on the diagonal show marginal density estimates. Figures below the diagonal show bivariate hexagonal histograms. The numbers above the diagonal indicate the Pearson correlation between the samples of the parameters.}
	\label{fig:baselinePosteriorDescriptives}
\end{figure}


\begin{figure}
	\caption{Convergence Diagnostics for the Analysis of the Baseline Data Set}
	\begin{nscenter}
		\begin{subfigure}{.5\textwidth}
			\centering
			\fitfigure{traceplotsBaseline}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\fitfigure{autocorrelationBaseline}
		\end{subfigure}%
	\end{nscenter}
	\figurenote{Left: trace plots of the first 10,000 posterior samples after warmup. The different chains appear indistinguishable, which indicates they converged. Right: Autocorrelation of the chains. The 0\textsuperscript{th} lag was omitted (as this is 1 by definition). The autocorrelation drops to 0 after about 5 iterations.}
	\label{fig:baselinePosteriorDiagnostics}
\end{figure}



\begin{figure}
	\caption{Visual Summary of the Posterior Distributions for the Group Level Effects of the Experimental Data Set.}
	\fitfigure{posteriorDescriptivesExperimental}
	\figurenote{The strips above and right of the figures indicate the parameters compared. Figures on the diagonal show marginal density estimates. Figures below the diagonal show bivariate hexagonal histograms. The numbers above the diagonal indicate the Pearson correlation between the samples of the parameters.}
	\label{fig:productPosteriorDescriptives}
\end{figure}


\begin{figure}
	\caption{Convergence Diagnostics for the Analysis of the Experimental Data Set.}
	\begin{nscenter}
		\begin{subfigure}{.5\textwidth}
			\centering
			\fitfigure{traceplotsExperimental}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\fitfigure{autocorrelationExperimental}
		\end{subfigure}%
	\end{nscenter}
	\figurenote{%
		Left: trace plots of the first 10,000 posterior samples after warmup. The different chains appear indistinguishable, which indicates they converged. Right: Autocorrelation of the chains. The 0\textsuperscript{th} lag was omitted (as this is 1 by definition). The autocorrelation drops to 0 after about 10 iterations.}
	\label{fig:experimentalPosteriorDiagnostics}
\end{figure}



\end{document}

