\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsfonts,amssymb, bm}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{apacite}

\usepackage{authblk}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

% custom commands
\usepackage{todonotes}
\setlength{\marginparwidth}{4cm}

\usepackage{setspace}
\doublespacing

\usepackage{pgfplotstable}
\pgfplotstableset{
	fixed zerofill,
	precision=3,
	col sep = comma,
	search path={../tables/}
}
\pgfkeys{/pgf/number format/precision={2}}%

\newcommand{\getVal}[3]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumber{\pgfplotsretval}%
}
\newcommand{\getValInt}[3]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumber[fixed, fixed zerofill=false]{\pgfplotsretval}%
}
\newcommand{\setVal}[4]{%
	\pgfplotstablegetelem{#1}{#2}\of{#3}%
	\pgfmathprintnumberto{\pgfplotsretval}{#4}%
}
\newcommand{\getCI}[2]{95\% HPD [\getVal{#1}{Lower}{#2}, \getVal{#1}{Upper}{#2}]}

\newcommand{\DON}	[1] 	{\todo[linecolor=gray, backgroundcolor=white]	{Don: 	{#1}}}
\newcommand{\DONa}	[1]		{\todo[inline, linecolor=gray, backgroundcolor=white]	{Don:	{#1}}}

\newcommand{\argument}[1]{\noindent\textbf{PG:} \textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}


\newcommand{\prob}[1]{p\left(#1\right)}
\newcommand{\lik}[1]{p\left(#1\right)}
\newcommand{\data}{\mathcal{D}}
\newcommand{\model}{\mathcal{M}}
\newcommand{\prior}[1]{\pi\left(#1\right)}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\Reals}{\mathcal{R}}
\newcommand{\dnorm}[2]{\text{Normal}\left(#1,\,#2\right)}


% title
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\author[1]{Don van den Bergh\thanks{Correspondence concerning this article should be addressed to
Don van den Bergh, University of Amsterdam, Department of Psychological Methods, Postbus 15906, 1001 NK Amsterdam, The Netherlands. E-Mail should be sent to donvdbergh@hotmail.com.}}
\author[2]{Nina Vandermeulen}
\author[2]{Rianne}
\author[2]{Marije Lesterhuis}
\author[2]{Sven de Maeyer}
\author[2]{Elke van Steendam}
\author[2]{Gert Rijlaarsdam}
\author[3]{Huub van den Bergh}
\affil[1]{University of Amsterdam}
\affil[2]{University of Antwerp}
\affil[3]{University of Utrecht}
% \setcounter{Maxaffil}{0}
% \renewcommand\Affilfont{\itshape\small}

\title{Priors Information for Multilevel models in Educational Analyses}
\date{}

\graphicspath{{../figures/}}

\begin{document}

\listoftodos
\newpage
\maketitle
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
    Scholastic achievement is often monitored in national assessments. For each topic, multiple tasks measure a student's skill, to avoid task-specific effects. For example, reading comprehension is measured with multiple texts. In experimental studies, a similar approach is often employed; because student performance varies across tasks, multiple tasks are administered so that the influence of an experimental condition may be assessed while controlling for the effect of task. National assessments consist of many tasks and the data is generally rich in information. In contrast, experimental studies typically use only one assignment, which makes it difficult to distinguish improvement in an experimental condition from between-task variance. This prompts the question of whether knowledge obtained from national assessments about between-task variance can be used in the analyses of experimental studies. Here, we demonstrate how the information of a baseline data set can be used in the analysis of an experimental study. We adopt a Bayesian paradigm as this enables us to propagate the uncertainty in the estimates of a national assessment into the analysis of the experimental study.
\end{abstract}
\newpage
\DONa{Misschien moeten we een naam verzinnen voor deze methode?}
%\section*{Introduction}

In many countries, the achievements of students are monitored in so-called national assessments. For instance, NAEP in the US, PEIL in the Netherlands (or international assessment programs like IEA or PIRLS) measure students' achievements at regular intervals to gain information on changes in achievement over time (or changes in differences between countries). For instance, in the Netherlands every four years the achievements of students are measured at the end of primary education in some of the most important subject areas. Although the results of these assessments often inform policymaking, the data are seldom used in educational research even though there are ample opportunities.

A common denominator in national assessments is for all subject areas measurements are based on an analysis of that subject area. Therefore, students read multiple texts if reading is assessed or write multiple texts if writing is assessed. This is a necessity if one wants to describe the level of achievements covering a whole domain while generalizing over specific assignments (or tests) at the same time. For writing in the Netherlands, for instance, students wrote 21 different texts in a national assessment \cite{zwarts1990balans}. Of course, not all students take every test, but a sparse design is in operation, to minimize testing time but allowing for conclusion at the population level at the same time.

If we contrast experimental studies with national assessments it is apparent that in many experimental studies the measurements are not as varied as in national assessments. In the vast majority of experimental studies on writing, students write one text as a pretest and one text as a posttest \cite<e.g.,>{graham2014conducting}. Based on these texts we are proned to draw conclusions on changes in the writing skills of students. Although it is well documented that differences between different types of writing assignments can be large \cite<e.g.,>{bouwer2015effect}, and we hardly can make inferences based on only one writing assignment. Of course, many researchers are aware of the limited generalizability of single-task experiments. However, it is often infeasible that students write more tasks.

%Hence, generalization over writing tasks does not appear to be a crucial issue in experimental writing studies as it is assessments.

So, on the one hand, there is much information on levels of achievement of students (at certain levels of education) from assessments, and on the other hand in many experimental studies, we rely on relatively small samples and relative narrow measures of skills. Therefore, one could wonder why don't we use the information from large scale assessments? Can this information from assessments be put to use in our experimental studies?

The information from national assessments can be seen as information on the level of achievements in general.
In this sense, this information might be seen as prior knowledge that describes the standard level of achievements. 
In experimental studies, we like to show that the increase in achievements due to the experimental manipulation exceeds `natural' growth.
Therefore, the information from assessments might function as a baseline, or standard level of achievement for experimental studies.
Second, the results of prior studies form the basis for new studies and research hypotheses.
Nevertheless, we do not fully use the available data.
Prior knowledge, prior data, is rarely used in statistical analyses.
This might be inefficient, as we keep measuring students over and over again to get studies that have enough power to draw conclusions.
However, we can also increase the power of studies if we enrich our analyses with prior results \cite{graham2014conducting}.
One of the types of studies that comes directly to mind are of course assessment studies, not only because many students take these tests, but also because students take many tests in order the generalize over the idiosyncrasies of specific tests.

Unfortunately, there exists no straightforward method to incorporate prior information into (frequentist) analyses. 
Ideally, the raw data from prior studies are included in the analyses as a benchmark comparison, but this is often impossible for practical (and privacy) reasons.
Alternatively, prior knowledge can be represented by treating the prior results as population values and experimental results can be tested against these values.
However, this approach seems far from ideal, as measurement error and uncertainty in the prior results are completely ignored.
Although such uncertainties could be introduced using standard errors, many types of frequentist analyses are not equipped for such procedures.

In this respect, Bayesian analyses of prior results from the data might be preferable; Bayesian statistics offer a rigorous and consistent approach to quantify uncertainty in statistical analyses. In Bayesian inference, prior knowledge (or a lack thereof) is represented by probability distributions, which describe all uncertainty about the quantities of interest. Upon observing the data, prior knowledge is updated to posterior knowledge, which is again represented by probability distributions. Key is that these probability distributions provide a complete account of the uncertainty. Thus, Bayesian inference is an ideal vehicle to reuse findings from prior analyses into future studies, while accounting for the uncertainty in these prior results. In educational research, there is an abundance of data, but results from the analyses are rarely used in the analysis of new studies.

The outline of this paper is as follows. 
First, we introduce a large data set on writing instruction in high school that serves as a baseline data set. 
Using this baseline data set, we provide a brief explanation of Bayesian statistics, before analyzing the data with a multilevel model. 
Next, we analyze a follow-up data set and relate the parameter estimates from the baseline analysis to those of the follow-up. 
The paper is concluded with a discussion on the widespread applicability and benefits of this approach and the limitations concerning the validity of this approach.

%We show how to approximate the posterior distributions with parametric distributions such that these can be used as prior distributions in future analyses. Next, we demonstrate the influence of these priors by analysing a follow up data set using both the newly obtained priors and uninformative priors. \DON{Some sentence about the diverence in results. The paper is concluded with a discussion on ... (todo).}


\section*{Baseline Data Set}
\noindent The baseline data set was collected to investigate the writing quality of students in the tenth, eleventh, and twelfth grade of high school. Here we provide some information about the data collection and some descriptives of the data.

Schools were selected at random by creating three lists of schools. First, a school in the first batch was approached for participating in the study. If this school did not reply or refused, a school in the second batch was selected at random. If the second school did not participate a school from the third batch was approached.

In total, the writing quality was measured for 625 students, nested in 43 schools. To assess between-task variance, 32 different tasks were administered of which students made four at random. Some students did not make all tasks, 497 students made four tasks and 128 students made three or fewer tasks. The minimum amount of students per task was 62 whereas the maximum was 84. Students' text were rated by... \DON{Aanvullen aub}.

The data contain an obvious nested structure, as illustrated in Figure~\ref{fig:baselineDescriptives}. Observations are nested within students and tasks. Furthermore, students are nested within schools. Students took a random sample of four tasks out of 32 tasks developed for this writing assessment.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{descriptivesBaseline.pdf}
	\caption{Box and whiskers plot of student performance on each task for the three grades measured. The grade is indicated above each panel and the task code is shown on the x-axis. There is substantial variance in student performance between tasks and within tasks, and student performance appears to increase in successive grades.}
	\label{fig:baselineDescriptives}
\end{figure}
The observations of text quality cannot be considered as independent.
Scores of students in the same school might be more alike than scores of students from different schools.
Likewise, scores on the same task might be more alike than scores on different writing tasks.
Therefore, a cross-classified multilevel model is in operation.
If $y_{(ij)k}$ is the score of student $i$ ($i = 1, 2, 3, ...., I_k$) on task $j$ ($j = 1, 2, ..., J_i$) in school $k$ ($k = 1, 2, ...., K$), we can write the model to be analyzed as:
\begin{align*}
	y_{(ij)k} = \beta \times \mathrm{Grade}_{ijk} + [w_{00k} + u_{i0k} + v_{0j0} + \epsilon_{(ij)k}].
\end{align*}
The model consists of two parts: a fixed part and a random part (between square brackets). In the fixed part, $\mathrm{Grade}_{ijk}$ is an indicator matrix for students' grade. Consequently, the vector of regression weights ($\beta$) represents the mean writing score for each grade. In the random part four residual scores are distinguished, all of which are assumed to be normally distributed around with an expected value of 0. The first residual ($w_{00k}$) captures the difference between a school and the average. The second residual ($u_{i0k}$) captures that the average score of student $i$ in school $k$ can deviate from the schools' mean. The third residual ($v_{0j0}$) captures that some tasks might be more difficult than other tasks. The fourth residual ($\epsilon_{(ij)k}$) indicates the deviation of the score of task $j$ of the average of student $i$ in school $k$. Usually, the variance of this term is interpreted as random noise.


\subsection*{Bayesian Inference}
\pgfmathsetmacro{\zCrit}{1.959964}% qnorm(0.975)

This section aims to give a brief introduction to Bayesian inference with an emphasis on the problem at hand. For a more elaborate introduction to Bayesian inference, see the recent special issue in \emph{Psychonomic Bulletin \& Review} which provides tutorials and guidance for aspiring Bayesians \cite{VandekerckhoveEtAl2018SI}. The choice for a Bayesian analysis is motivated by the fact that Bayesian inference is naturally accompanied by uncertainty estimates, as is explained later. Thus, the estimates of a baseline study and an experimental study can be compared while accounting for the uncertainty in both sets of estimates.

Bayesian inference is centered on the updating of beliefs. For any parameter in a given statistical model $\model$, the values this parameter can take are assigned a prior belief. These beliefs are represented with a probability distribution, usually called the prior distribution $\pi$. For example, in a multilevel model, the intercept $\beta_0$ can be assigned a normal distribution as prior distribution with mean 0 and variance 1. Then the a-priori the most likely values for the intercept are near 0 and about 95\% of the prior mass lies within \pgfmathprintnumber{-\zCrit} and \pgfmathprintnumber{\zCrit}.

The key step in Bayesian inference is to use the data $\data$ to update the prior beliefs to posterior beliefs. The procedure for updating the prior distribution to a posterior distribution is given by Bayes theorem:
\begin{align*}\label{eq:BayesTheorem}
\underbrace{\prob{\bm{\beta} \mid \data , \model}}_{\text{Posterior}}
&=
\overbrace{\prior{\bm{\beta}\mid \model}}^{\text{Prior}}
%\enspace \times %\enspace
\underbrace{\overbrace{
		\frac{\lik{\data \mid \bm{\beta}, \model}}{\prob{\data \mid \model}}
	}^{\text{Likelihood}}}_{\substack{\text{Marginal}\\ \text{Likelihood}}}.
\end{align*}
Here, $\bm{\beta}$ represents all parameters in the model. The prior distribution of the parameters is updated through the likelihood of the statistical model. The likelihood is divided by the marginal likelihood so that the posterior distribution is a proper probability distribution (i.e., it integrates to 1). The posterior distribution is key for parameter estimates. For instance, if a single estimate for a parameter is desired, one could use the mean of the posterior distribution. Other often-used point-estimates are the posterior mode and posterior median. Simultaneously with obtaining the posterior, a measure of uncertainty for each parameter is obtained. Since the posterior distribution is a proper probability distribution, we can make inferences about the parameters. For example, given the posterior distribution for the intercept, $\prob{\beta_0 \mid \data , \model}$. This implies questions such as ``Given that we have seen the data, what is the probability that the intercept is larger than 0?'' Can be answered by computing $\prob{\beta_0 > 0 \mid \data , \model}$. Likewise, if we find a lower bound $LB$ and upper bound $UB$ for the intercept $\beta_0$ such that $\prob{ LB \leq \beta_0 \leq UB \mid \data , \model} = 0.95$, we can claim: ``Given that we have seen the data, we are $95\%$ confident that the true value of the intercept lies between $LB$ and $UB$.'' This interval is known as the Bayesian $95\%$ credible interval. Another often-used Bayesian uncertainty interval is the 95\% highest posterior density interval (HPD), an interval that contains 95\% of the posterior mass and has the highest probability density.

\subsubsection*{Approximations to Posterior Distributions}
Although Bayes theorem may appear straightforward, in practice the posterior distribution can be a high-dimensional probability distribution that is difficult to study analytically. Rather than studying the mathematical form of the posterior, it is much easier to simulate random values from the posterior distribution and to use these for inference. Such simulation methods are commonly referred to as Markov chain Monte Carlo (MCMC). The idea is that instead of computing a statistic of the posterior distribution in closed form, we can draw random observations from the posterior and use a sample estimator to approximate the statistic of the distribution. For example, if we are interested in the posterior mean of the intercept, we simulate many observations from the posterior distribution and use the sample mean of these observations to approximate the posterior mean of the intercept. Likewise, to compute the posterior probability that an intercept $\beta_0$ is positive, $\prob{\beta_0 > 0 \mid \data , \model}$, we examine the proportion of MCMC samples where $\beta_0$ is positive. This procedure is akin to how applied scientists attempt to randomly sample participants from a population and then generalize the sample statistics to the entire population, with the exception that is it relatively easy to draw enormous samples with MCMC to obtain near-perfect approximations.

\subsection*{Statistical Software}
\pgfplotstableread{mcmcSettings.csv}\tbMCMCsettings

All analyses were done in R \cite{R}. The R package \code{brms} was used for Bayesian multilevel analyses \cite{burkner2017brms}. The R package \code{brms} is a convenient front-end for the probabilistic programming language Stan, which is software for general-purpose Bayesian inference \cite{carpenter2017stan}. For all analyses, we used six MCMC chains to assess convergence. Convergence was assessed using the $\widehat{R}$ statistic \cite{vehtari2019rank}. In line with the recommendations by \citeA{vehtari2019rank}, we tweaked the parameters of the Stan algorithm such that the $\widehat{R}$ is less than 1.01 and the rank-normalized effective sample size is larger than 400. Per chain, we simulated \getValInt{0}{iter}{\tbMCMCsettings} samples and discarded the first \getValInt{0}{warmup}{\tbMCMCsettings} as warmup samples. In total, results in Tables and Figures are based on \getValInt{0}{total}{\tbMCMCsettings} samples.

We used the default prior distributions of the R package \code{brms} for all parameters. That is, the standard deviations of the random effects and the residual were assigned a half t-distribution with a mean of 0, scale of 18, and 3 degrees of freedom. For the fixed effects we used a Cauchy distribution with location 0 and scale 1.

\section*{Baseline Analysis}

% setup table
\pgfplotstableread{postSummaryBaseline.csv}\tbPostSummaryBaseline

We summarized the posterior distribution in Table~\ref{tb:baselineSummary}.
This shows that the average text quality of students in grade 10 is estimated at \getVal{0}{Mean}{\tbPostSummaryBaseline}.
The 95\% highest posterior density (HPD) credible interval ranges from  \getCI{0}{\tbPostSummaryBaseline}.
Students in grade 11 performed on average about \getVal{1}{Mean}{\tbPostSummaryBaseline} points better (\getCI{1}{\tbPostSummaryBaseline}) than students in grade 10.
Likewise, students in grade 12 performed on average about \getVal{2}{Mean}{\tbPostSummaryBaseline} points better (\getCI{2}{\tbPostSummaryBaseline}) than students in grade 10.
However, the estimated variance between schools (\getVal{3}{Mean}{\tbPostSummaryBaseline}), students within school (\getVal{4}{Mean}{\tbPostSummaryBaseline}), and tasks (\getVal{5}{Mean}{\tbPostSummaryBaseline}) clearly deviate from 0.
Since the data set contained such a large variety of schools and tasks, these findings likely generalize over tasks.
\begin{table}[!ht]
	\caption{Summary of the posterior distribution for the baseline data set. The first column shows the parameter. The second the posterior mean for that parameter, the third the posterior standard deviation and the last two columns show the 95\% higher posterior density interval. Grade 11 and 12 represent the improvement relative to grade 10 (the intercept).}
	\label{tb:baselineSummary}
	\centering
	\pgfplotstabletypeset[
	column type=r,
	every head row/.style={
		before row={
			\toprule
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{95\% HPD} \\
			\cmidrule[0.4pt]{4-5}
		},
		after row=\midrule,
	},
	every last row/.style={
		after row=\bottomrule
	},
	columns/Parameter/.style={string type}
	]\tbPostSummaryBaseline
\end{table}

Figure~\ref{fig:baselinePosteriorTextQuality} visualizes the improvement in text quality across grades. To obtain the posterior distributions for grades 11 and 12, we add the posterior distribution of the intercept to that of the improvement in Grade 11 and Grade 12.
\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{posteriorTextQualityBaseline}
	\caption{Posterior distribution of text quality in grades 10, 11, and 12. Posteriors distributions for grades 11 and 12 are obtained by adding the MCMC samples of the intercept to the MCMC samples for the improvement of the respective grade.}
	\label{fig:baselinePosteriorTextQuality}
\end{figure}

\section*{Application to an Experimental Analysis}

\subsection*{Data Set}
Data were collected from 89 students of two high-schools in the Netherlands. Students made three writing tasks in one week; one on Monday, Wednesday, and Friday. After the first task, the students received feedback on the quality of their written texts by a rating scale based on the baseline data set.
\DON{Aanvullen aub!}

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{descriptivesExperimental}
	\caption{Box and whiskers plot of student performance on the three measurement occasions. Grey points represent the raw scores on text quality. Quasi-random jitter was added to the x-coordinates of the points to avoid visual clutter. The average performance clearly increases from measurement one to two, but it is hard to quantify the improvement without a reference group.}
	\label{fig:productDescriptives}
\end{figure}

\subsection*{Analysis}
\pgfplotstableread{postSummaryExperimental.csv}\tbPostSummaryProduct

A typical analysis for this data set is almost identical to that of the baseline data set, except that here we estimate differences between measurements, which might be contaminated with differences due to tasks. Since each student took only one task at each measurement occasion, the between-task variance cannot be estimated. Thus $y_{(hi)k}$ is the observation of measurement $h$ ($h=1, 2, 3$) of student $i$ ($i=1, ..., K_i$) in school $k$ ($k = 1, 2$). The multilevel model thus becomes:
\begin{align*}
y_{(ij)k} = \beta_0 + \beta \times \mathrm{Measurement}_{ijk} +  [w_{00k} + u_{i0k} + \epsilon_{ijk}].
\end{align*}
Here, $y_{ijk}$ is the observation of student $i$ on measurement $j$ in school $k$. The fixed part consists of an intercept ($\beta_0$), fixed effect of measurement $\beta$. The random part consists of a random intercept for school ($w_{00k}$), a random intercept for person within school $u_{i0k}$ and a residual $\epsilon_{ijk}$.
%\begin{align*}
%y_{psm} &= \beta_0 + \beta_p + \beta_s + \beta_m  + \epsilon_{pstg}.
%\end{align*}
%Here, the observation of student $p$ in school $s$ on measurement $m$ is denoted $y_{psm}$. The score consists of an intercept $\beta_0$, a random intercept for person $\beta_p$ and school $\beta_s$, and a fixed effect of measurement $\beta_m$.
As for the baseline analysis, we summarize the posterior distribution of the multilevel model using the mean, standard deviation, and HPD in Table~\ref{tb:productPosteriorSummary}. This shows that the average text quality is estimated at \getVal{0}{Mean}{\tbPostSummaryProduct} (\getCI{0}{\tbPostSummaryProduct}). At the second measurement occasion, students performed on average about \getVal{1}{Mean}{\tbPostSummaryProduct} points better (\getCI{1}{\tbPostSummaryProduct}) than at intake. At follow up, students' improvement was estimated at \getVal{2}{Mean}{\tbPostSummaryProduct} (\getCI{2}{\tbPostSummaryProduct}). A bivariate scatterplot for the parameters in Table~\ref{tb:productPosteriorSummary} is shown in Figure~\ref{fig:productPosteriorDescriptives}.

\begin{table}[!ht]
	\caption{Summary of the posterior distribution for the experimental data set. The first column shows the parameter. The second the posterior mean for that parameter, the third the posterior standard deviation and the last two columns show the 95\% higher posterior density interval. The improvement of measurement 2 and 3 is relative to the intercept (measurement 1).}
	\label{tb:productPosteriorSummary}
	\centering
	\pgfplotstabletypeset[
		column type=r,
		every head row/.style={
			before row={
				\toprule
				\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{95\% HPD} \\
				\cmidrule[0.4pt]{4-5}
			},
			after row=\midrule,
		},
		every last row/.style={
			after row=\bottomrule
		},
		columns/Parameter/.style={string type}
	]\tbPostSummaryProduct
\end{table}

The estimated improvement across measurement occasions is shown in Figure~\ref{fig:productPosteriorTextQual}. Apparent is that students perform better at post-test than at pre-test and that the difference between follow up and post-test appears negligible.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{posteriorTextQualityExperimental.pdf}
	\caption{Posterior distribution of text quality at each measurement occasion of the product data set.}
	\label{fig:productPosteriorTextQual}
\end{figure}
At this point in the analysis, drawing conclusions about the effect of the treatment is problematic because there is no control group. Thus, the improvement of the students cannot solely be attributed to just the intervention but might be caused by differences in difficulty between tasks.
%Thus, it is difficult to interpret the improvement of the students over the measurement occasions, as variation in the students' performance may be caused by between task variance. Given the


%\subsection*{Relating Baseline Result to Experimental Findings}
\subsection*{Relating Baseline Results to the Analysis of an Experimental Study}
\pgfplotstableread{postMeansCorrectedExperimental.csv}\tbPostMeansProdCC

Ideally, we directly compare the difference in text quality between measurement occasions in the experimental study. However, interpreting these differences is not straightforward as the contamination of task effect and measurement occasion make this impossible. To make the differences between measurement occasions interpretable we need to correct these for task difficulty. As the baseline study provides estimates of task difficulty, a correction is self-evident.  We can correct students' scores in the experimental study by subtracting the estimated task in the baseline study. As a consequence, the corrected posterior means for each measurement occasion changed slightly, see Figure~\ref{fig:comparePostTextQual} (from \getVal{0}{Mean}{\tbPostMeansProdCC} to \getVal{3}{Mean}{\tbPostMeansProdCC} for measurement 1, from \getVal{1}{Mean}{\tbPostMeansProdCC} to \getVal{4}{Mean}{\tbPostMeansProdCC} for measurement 2, and from \getVal{2}{Mean}{\tbPostMeansProdCC} to \getVal{5}{Mean}{\tbPostMeansProdCC} for measurement 3). Note that a direct comparison is possible because the rating procedure of the experimental study is based on the rating procedure of the baseline study.
\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{comparePosteriorTextQuality.pdf}
	\caption{Posterior distributions for each grade in the baseline study and each measurement occasion in the experimental study. The posterior distributions of the baseline study are more narrow because they are based on more observations. We subtracted the estimated average task effect of each task category in the baseline study from the posteriors distributions in the experimental study to correct these for task effect.}
	\label{fig:comparePostTextQual}
\end{figure}

From Figure~\ref{fig:comparePostTextQual} we can infer that the corrected difference between measurement 1 and measurement 2 in the experimental study is almost as large as the difference between grade 10 and 11 in the baseline study. Hence, there is a substantial effect. By comparison, the difference between measurement 2 and measurement 3 is much smaller. Of course, this is not a statistical test of significance. Typically, such a test should account for between-task variance. To obtain an estimate for the magnitude of between-task effects we can use the estimates of the baseline study to simulate a distribution of task difficulty. Next, we can compute the probability that the observed difference between measurement occasions in the experimental study is due to differences between tasks.

Since \DON{tot hier gekomen!}multilevel models typically assume that the random effects follow a normal distribution with mean 0 we simulate a large number these from a normal distribution. As variance, we use the posterior samples for the between-task variance, to propagate the uncertainty in this parameter into the distribution over task-effects.\footnote{Essentially, the effects of these new random tasks are drawn from the posterior predictive distribution of the baseline study.} In total \getValInt{0}{total}{\tbMCMCsettings} task-effects were sampled (the same amount as MCMC samples). Next, we can visualize the posterior distribution of improvement between measurement occasions and contrast this to the distribution over task-effects, as shown in Figure~\ref{fig:posteriorImprovement}.
\begin{figure}[!ht]
	\centering
%	\begin{subfigure}{.33\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{compareTaskEffectToT2.pdf}
%	\end{subfigure}%
%	\begin{subfigure}{.33\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{compareTaskEffectToT3.pdf}
%	\end{subfigure}%
%	\begin{subfigure}{.33\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{compareTaskEffectToT23.pdf}
%	\end{subfigure}%
	\includegraphics[width=\textwidth]{compareTaskEffects.pdf}
	\caption{Distribution of the effect of a random task (grey) versus the posterior distribution of the estimated progress in the experimental data. The blue density in the left panel is the posterior distribution of improvement between the first and second measurement; the purple density in the middle panel is the posterior distribution of improvement between the first and third measurement; the dark blue density in the right panel is the posterior distribution of improvement between the second and third measurement. The probability that a random task is large than the improvement is shown above each panel.}
	\label{fig:posteriorImprovement}
\end{figure}

The left and middle panel in Figure~\ref{fig:posteriorImprovement} contrast measurement occasions 2 and 3 against intake.
The improvement appears to exceeds what would be expected of a random task-effect.
The right panel contrasts measurement 2 with measurement 3. Here, the improvement seems indistinguishable from a random task-effect.
This makes sense as there was no intervention between measurements 2 and 3.


The above results show that the observed effect between measurement occasions 1 and 2 is greater than can be expected from any given task.
However, this does not provide a straightforward manner to interpret the magnitude of this effect.
To obtain a measure that is easy to interpret, we again compare the results to that of the baseline.
From the baseline study, we obtained a posterior distribution that quantifies students' improvement between grade 10 and grade 11, accounting for differences between tasks (i.e., parameter Grade 11 in Table~\ref{tb:baselineSummary}).
Next, we take the posterior samples for the effect of measurement in the experimental study (Parameter Measurement 2 in Table~\ref{tb:productPosteriorSummary}) and divide these by the samples of the baseline study.
The resulting posterior distribution expresses the progress of students in the experimental study in baseline study years and provides a practically intuitive interpretation for the effect size.
The resulting posterior distributions are shown in Figure~\ref{fig:improvementInYears}.

% these cannot be generated inside the caption
\pgfplotstableread{credibleIntervalsImprovement.csv}\tbCRIimprovement
\setVal{0}{Lower}{\tbCRIimprovement}{\criLowerA}
\setVal{0}{Upper}{\tbCRIimprovement}{\criUpperA}
\setVal{1}{Lower}{\tbCRIimprovement}{\criLowerB}
\setVal{1}{Upper}{\tbCRIimprovement}{\criUpperB}
\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{improvementInYears.pdf}
	\caption{Improvement in the experimental study relative to the improvement between grade 11 and grade 10. The left panel shows the posterior distribution for the improvement between measurement 1 and measurement 2 divided by the improvement from grade 10 to 11 (95\% HPD [$\criLowerA$, $\criUpperA$]). The right panel shows the improvement from measurement 1 to measurement 3, standardized in the same manner (95\% HPD [$\criLowerB$, $\criUpperB$]). The shaded areas under the curve represent 95\% HPD intervals.}
	\label{fig:improvementInYears}
\end{figure}

In the left panel of Figure~\ref{fig:improvementInYears}, the posterior has a mean \getVal{0}{mean}{\tbCRIimprovement} (\getCI{0}{\tbCRIimprovement}), which indicates that the students appear to have gained almost a year in ability between the two measurements.  

\section*{Discussion}
%\DONa{verwerkt dit comment in de discussie: om didactische redenen lijkt het me idd goed om het onderscheid tussen de experimenten er avn buiten te laten, maar aan het eind zou het een mooi sluitstuk zijn als je wel laat zien hoe je dan met twee xperimentele condities kunt werken. Voordeel van de beshcikking van een baseline is dat je niet een controlegroep hoeft mee te nemen, maar dat je tijd en geld kunt besteden aan een theoretisch interessante experimentele tegenhanger.}
In this paper, we introduced a procedure for comparing results from a large scale assessment into the analysis of an experimental study that lacked a control group.
If a control group is missing, the task-effect cannot be disentangled from the effect of an intervention.
However, by relating the increase in performance to estimates of the between-task variance in a baseline study, we can compute how probable it is that improvement across measurements is a task-effect.
This method could provide a point of reference for studies without a control group and may help discern between statistically significant effects and practically relevant effects
\cite{hojat2004visitor, fan2001statistical}.

Comparing the distribution of task-effect in an experimental study to that of a baseline study relates to approaches of statistical tests for equivalence, such as TOST \cite{lakens2017equivalence}, ROPE \cite{kruschke2011bayesian}, and interval Bayes factors \cite{MoreyRouder2011}.
These three approaches have in common that a researcher specifies some minimal effect size below which an effect is practically equivalent to zero.
Another thing these tests have in common is that they provide little guidance on how to determine such a minimal effect size.
In contrast, our approach can be seen as deriving this minimal effect size from a baseline study (e.g., the effect size that is sufficiently implausible to be caused by between task effects.)

Here, we opted to speak of significance when the posterior probability that the observed task-effect is larger than that of a random task is less than $0.05$. This choice is arbitrary and other motivations have been suggested \cite{McShane2017abandon, BenjaminEtAl2018}.

We chose for a Bayesian analysis because it allows us to account for the uncertainty in the estimates of the baseline study when comparing these to the results in the experimental study. In a frequentist analysis, it is also possible to do this so some extent. A distribution of effect sizes can be obtained by using the point estimates


\subsection*{Limitations}
\DONa{we negeren hier de steekproef variantie in de experimentele analyse. Dwz, we weten niet hoe waarschijnlijk het is dat we, gegeven H0, een effect vinden dat groter is dan die in de baseline.}
It is important to stress that if improvement exceeds between-task effects the results cannot be viewed as a causal relation.
This is no different from ``correlation is not causation''; since no randomized experiment with a control group takes place the results are correlational and must be interpreted as such.
To assert a causal relation a control group is essential.
%
%we must
%Although, between-task variance is a major source of variance, and we can quantify how unlikely it is that the results are caused by between-task variability.
%Nonetheless, to assert a causal relation between an intervention and an increase in performance a control group is required.

A key assumption is that task-effects are, at least asymptotically, normally distributed.
If normality is violated then the probabilities shown in Figure~\ref{fig:posteriorImprovement} could be biased.
Here, we briefly outline an argument on why the task effects are likely approximately normally distributed.
Note that a naive estimator for the effect of a task is simply the mean of the students' scores on that task.
Although this estimator is unbiased, much better estimates can be obtained in practice by accounting for the hierarchical structure \cite<e.g., see>{EfronMorris1977}.
Since the naive estimator is an average the central limit theorem applies and thus the distribution of task-effects converges asymptotically to a normal distribution (under mild regularity conditions).
\DON{ik weet niet zeker of dit argument helemaal klopt.}

Another avenue for incorporating the results of a baseline study into the analysis of an experimental study is through the prior distribution.
The posterior distribution of the baseline study could serve as the prior distribution for the experimental study.
Although this is conceptually straightforward, we did not do so for two reasons.
First, to obtain exact approximations to the posterior, the analyst of the experimental study must have the original data to obtain posterior distributions for the baseline data set.
In practice, it is unlikely that an analyst has access to a baseline data set which limits the applicability of the method.
It is possible to approximate the marginal posterior distributions using some parametric family of distributions, which can then be published and used in experimental studies.
However, these approximations will likely ignore the correlations and other higher-order moments in the posterior distribution.
The consequences of ignoring the higher-order moments in the posterior distribution are simply unknown.
Second, the benefit of informed priors is unclear, as the data typically overwhelm the influence of the prior distribution, barring extreme cases \cite{Lynch2007}.
Thus, since the inferences done in the paper are based solely on the posterior distribution, the influence of the prior distribution is likely negligible.
%A clear use case for these priors is typical Bayesian hypothesis.
%Typically, Bayesian hypothesis testing is done using Bayes factors, which are sensitive to the choice of the prior distribution (in particular the marginal likelihood is sensitive to the prior \citeNP{gelman2017prior}).
%Prior distributions informed by baseline studies could facilitate
%Third, applying


\DONa{We kunnen vertellen dat frequentisten dit ook kunnen doen met een puntschatting van variantie. Het is dan alleen niet duidelijk is hoe de standaardfout van die puntschatting gebruikt kan worden om de onzekerheid van de schatting te laten voortvloeien in de gesimuleerde effect groottes (en het negeren van de onzekerheid zal leiden tot een te nauwe verdeling van effect groottes en als gevolg te zekere uitspraken gegeven de data).}

\subsection*{Recommendations}
Prior information can enrich the statistical analyses and provide more insight into the data.
Here, we outline several recommendations for those who wish to apply our method for incorporating prior information in practice.


A key requirement for comparing results from a large scale assessment with those from an experimental study is that the data are comparable.
Whether the data are comparable hinges on the validity of the measurement instruments.
It is not so much important that the instruments measure what they are supposed to measure (the traditional definition of validity), but rather they should measure the same construct.
If the baseline study measured different constructs than the experimental study, for instance, because they used different measurement instruments, then a comparison is unintelligible and thus meaningless.
Thus, we recommend using the same measurement instruments as those used in a baseline assessment.

The use of a baseline study instead of a control group opens up new avenues for designing experimental studies.
Currently, researches tend to allocate about half of the available resources to a control group and the other half to an experimental group.
However, since the experimental group can now be related to a baseline study, it becomes possible to allocate funds to a theoretically competing theory, rather than to a control group.

The use of our method is subject to large scale assessments publishing their results.
It is key that those studies either disclose the raw data or that they publish the marginal posterior distributions of the parameters.
If the results of large scale assessments are not available as a benchmark, then it is inoperable to use them to inform the analysis of experimental studies.




In sum, we related the results from a baseline study to the analysis of an experimental study that lacked a control group.
This allowed us to determine whether the differences between measurements in the experimental group exceeded what would be expected from between task variance.
Altogether, this may help to place effect sizes of experimental studies in a broader context.
\DONa{Hier moeten we iets meer positieve dingen zeggen over wat voor mogelijkheden deze methode introduceert.}

\bibliographystyle{apacite}
\bibliography{references}

\newpage

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{posteriorDescriptivesBaseline}
	\caption{A visual summary of the posterior distributions for the group level effects of the baseline data set. The strips above and right of the figures indicate the parameters compared. Figures on the diagonal show marginal density estimates. Figures below the diagonal show bivariate hexagonal histograms. The numbers above the diagonal indicate the Pearson correlation between the samples of the parameters.}
	\label{fig:baselinePosteriorDescriptives}
\end{figure}


\begin{figure}[!ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{traceplotsBaseline}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{autocorrelationBaseline}
	\end{subfigure}%
	\caption{
		Convergence diagnostics for the analysis of the baseline data set. Left: trace plots of the first 10,000 posterior samples after warmup. The different chains appear indistinguishable, which indicates they converged. Right: Autocorrelation of the chains. The 0\textsuperscript{th} lag was omitted (as this is 1 by definition). The autocorrelation drops to 0 after about 5 iterations.}
	\label{fig:baselinePosteriorDiagnostics}
\end{figure}



\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{posteriorDescriptivesExperimental}
	\caption{A visual summary of the posterior distributions for the group level effects of the experimental data set. The strips above and right of the figures indicate the parameters compared. Figures on the diagonal show marginal density estimates. Figures below the diagonal show bivariate hexagonal histograms. The numbers above the diagonal indicate the Pearson correlation between the samples of the parameters.}
	\label{fig:productPosteriorDescriptives}
\end{figure}


\begin{figure}[!ht]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{traceplotsExperimental.pdf}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{autocorrelationExperimental}
	\end{subfigure}%
	\caption{
		Convergence diagnostics for the analysis of the experimental data set.
		Left: trace plots of the first 10,000 posterior samples after warmup. The different chains appear indistinguishable, which indicates they converged. Right: Autocorrelation of the chains. The 0\textsuperscript{th} lag was omitted (as this is 1 by definition). The autocorrelation drops to 0 after about 5 iterations.}
	\label{fig:baselinePosteriorDiagnostics}
\end{figure}



\end{document}

